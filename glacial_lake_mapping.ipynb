{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "glacial_lake_mapping.ipynb",
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyODpXLHjJyHWOB1tr5MTkod",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gittu-Thampi/glacial_lake_mapping/blob/main/glacial_lake_mapping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "LRL7ykGtWb82"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XNF4AB8aS9Qf"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import partial\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from skimage.filters import threshold_otsu\n",
        "from tensorflow.keras.metrics import MeanIoU\n",
        "from tqdm import tqdm\n",
        "from pylab import *"
      ],
      "metadata": {
        "id": "Xf1Ojn25Wlgj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images_dir = \"/content/drive/MyDrive/programs/GlacialMaping/WaterBodies/Images\"\n",
        "masks_dir = \"/content/drive/MyDrive/programs/GlacialMaping/WaterBodies/Masks\"\n",
        "\n",
        "dirname, _, filenames = next(os.walk(images_dir))"
      ],
      "metadata": {
        "id": "PjgeHhutWuPM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def load_img_with_mask(image_path, images_dir: str = 'Images', masks_dir: str = 'Masks', images_extension: str = 'jpg',\n",
        "                       masks_extension: str = 'jpg') -> dict:\n",
        "    image = tf.io.read_file(image_path)\n",
        "    image = tf.image.decode_jpeg(image, channels=3)\n",
        "\n",
        "    mask_filename = tf.strings.regex_replace(image_path, images_dir, masks_dir)\n",
        "    mask_filename = tf.strings.regex_replace(mask_filename, images_extension, masks_extension)\n",
        "    mask = tf.io.read_file(mask_filename)\n",
        "    mask = tf.image.decode_image(mask, channels=1, expand_animations=False)\n",
        "    return (image, mask)\n",
        "\n"
      ],
      "metadata": {
        "id": "jEefEyv8Wwp1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.function\n",
        "def resize_images(images, masks, max_image_size=1500):\n",
        "    shape = tf.shape(images)\n",
        "    scale = (tf.reduce_max(shape) // max_image_size) + 1\n",
        "    target_height, target_width = shape[-3] // scale, shape[-2] // scale\n",
        "    images = tf.cast(images, tf.float32)\n",
        "    masks = tf.cast(masks, tf.float32)\n",
        "    if scale != 1:\n",
        "        images = tf.image.resize(images, (target_height, target_width), method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
        "        masks = tf.image.resize(masks, (target_height, target_width), method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
        "    return (images, masks)\n"
      ],
      "metadata": {
        "id": "cIBe9Y9oXFnA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def scale_values(images, masks, mask_split_threshold=128):\n",
        "    images = tf.math.divide(images, 255)\n",
        "    masks = tf.where(masks > mask_split_threshold, 1, 0)\n",
        "    return (images, masks)\n"
      ],
      "metadata": {
        "id": "tn3Fc3GjXCOm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def pad_images(images, masks, pad_mul=16, offset=0):\n",
        "    shape = tf.shape(images)\n",
        "    height, width = shape[-3], shape[-2]\n",
        "    target_height = height + tf.math.floormod(tf.math.negative(height), pad_mul)\n",
        "    target_width = width + tf.math.floormod(tf.math.negative(width), pad_mul)\n",
        "    images = tf.image.pad_to_bounding_box(images, offset, offset, target_height, target_width)\n",
        "    masks = tf.cast(tf.image.pad_to_bounding_box(masks, offset, offset, target_height, target_width), tf.uint8)\n",
        "    return (images, masks)\n",
        "    "
      ],
      "metadata": {
        "id": "TF_g5KxGW_1v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "test_set_size = 300\n",
        "validation_set_size = 250\n",
        "\n",
        "dataset = tf.data.Dataset.list_files(images_dir + '/*.jpg', seed=42)\n",
        "\n",
        "test_dataset = dataset.take(test_set_size)\n",
        "dataset = dataset.skip(test_set_size)\n",
        "test_dataset = test_dataset.map(load_img_with_mask)\n",
        "test_dataset = test_dataset.map(scale_values)\n",
        "test_dataset = test_dataset.shuffle(20)\n",
        "test_dataset = test_dataset.map(lambda img, mask: resize_images(img, mask, max_image_size=2500))\n",
        "test_dataset = test_dataset.map(pad_images)\n",
        "test_dataset = test_dataset.batch(1).prefetch(5)\n",
        "\n",
        "validation_dataset = dataset.take(validation_set_size)\n",
        "train_dataset = dataset.skip(validation_set_size)\n",
        "validation_dataset = validation_dataset.map(load_img_with_mask)\n",
        "validation_dataset = validation_dataset.map(scale_values)\n",
        "validation_dataset = validation_dataset.shuffle(20)\n",
        "validation_dataset = validation_dataset.map(resize_images)\n",
        "validation_dataset = validation_dataset.map(pad_images)\n",
        "validation_dataset = validation_dataset.batch(1).prefetch(5)\n",
        "\n",
        "train_dataset = train_dataset.map(load_img_with_mask)\n",
        "train_dataset = train_dataset.map(scale_values)\n",
        "train_dataset = train_dataset.shuffle(20)\n",
        "train_dataset = train_dataset.map(resize_images)\n",
        "train_dataset = train_dataset.map(pad_images)\n",
        "train_dataset = train_dataset.batch(1).prefetch(5)\n",
        "\n",
        "def get_unet(hidden_activation='relu', initializer='he_normal', output_activation='sigmoid'):\n",
        "    PartialConv = partial(keras.layers.Conv2D,\n",
        "                          activation=hidden_activation,\n",
        "                          kernel_initializer=initializer,\n",
        "                          padding='same')"
      ],
      "metadata": {
        "id": "sta4OB_1XVFL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoder\n",
        "    model_input = keras.layers.Input(shape=(None, None, 3))\n",
        "    enc_cov_1 = PartialConv(32, 3)(model_input)\n",
        "    enc_cov_1 = PartialConv(32, 3)(enc_cov_1)\n",
        "    enc_pool_1 = keras.layers.MaxPooling2D(pool_size=(2, 2))(enc_cov_1)\n",
        "\n",
        "    enc_cov_2 = PartialConv(64, 3)(enc_pool_1)\n",
        "    enc_cov_2 = PartialConv(64, 3)(enc_cov_2)\n",
        "    enc_pool_2 = keras.layers.MaxPooling2D(pool_size=(2, 2))(enc_cov_2)\n",
        "\n",
        "    enc_cov_3 = PartialConv(128, 3)(enc_pool_2)\n",
        "    enc_cov_3 = PartialConv(128, 3)(enc_cov_3)\n",
        "    enc_pool_3 = keras.layers.MaxPooling2D(pool_size=(2, 2))(enc_cov_3)\n",
        "\n",
        "    # Center\n",
        "    center_cov = PartialConv(256, 3)(enc_pool_3)\n",
        "    center_cov = PartialConv(256, 3)(center_cov)\n",
        "\n",
        "    # Decoder\n",
        "    upsampling1 = keras.layers.UpSampling2D(size=(2, 2))(center_cov)\n",
        "    dec_up_conv_1 = PartialConv(128, 2)(upsampling1)\n",
        "    dec_merged_1 = tf.keras.layers.Concatenate(axis=3)([enc_cov_3, dec_up_conv_1])\n",
        "    dec_conv_1 = PartialConv(128, 3)(dec_merged_1)\n",
        "    dec_conv_1 = PartialConv(128, 3)(dec_conv_1)\n",
        "\n",
        "    upsampling2 = keras.layers.UpSampling2D(size=(2, 2))(dec_conv_1)\n",
        "    dec_up_conv_2 = PartialConv(64, 2)(upsampling2)\n",
        "    dec_merged_2 = tf.keras.layers.Concatenate(axis=3)([enc_cov_2, dec_up_conv_2])\n",
        "    dec_conv_2 = PartialConv(64, 3)(dec_merged_2)\n",
        "    dec_conv_2 = PartialConv(64, 3)(dec_conv_2)\n",
        "\n",
        "    upsampling3 = keras.layers.UpSampling2D(size=(2, 2))(dec_conv_2)\n",
        "    dec_up_conv_3 = PartialConv(32, 2)(upsampling3)\n",
        "    dec_merged_3 = tf.keras.layers.Concatenate(axis=3)([enc_cov_1, dec_up_conv_3])\n",
        "    dec_conv_3 = PartialConv(32, 3)(dec_merged_3)\n",
        "    dec_conv_3 = PartialConv(32, 3)(dec_conv_3)\n",
        "\n",
        "    output = keras.layers.Conv2D(1, 1, activation=output_activation)(dec_conv_3)\n",
        "\n",
        "    return tf.keras.Model(inputs=model_input, outputs=output)"
      ],
      "metadata": {
        "id": "lwoqJs8OXmSN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_unet()\n",
        "\n",
        "optimizer = tf.keras.optimizers.Nadam()\n",
        "model.compile(loss='binary_crossentropy', optimizer=optimizer)"
      ],
      "metadata": {
        "id": "ZaC_sCuwXyMe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "QJkoJC0OX8hK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
        "lr_reduce = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=3, verbose=1)\n",
        "\n",
        "epochs = 10\n",
        "history = model.fit(train_dataset, validation_data=validation_dataset, epochs=epochs, callbacks=[early_stopping, lr_reduce])"
      ],
      "metadata": {
        "id": "JOgf5AtOX9tA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def OtsuFilter(prediction, bias=0):\n",
        "    threshold = threshold_otsu(prediction) - bias\n",
        "    \n",
        "    return prediction > threshold"
      ],
      "metadata": {
        "id": "eCgU0mFeZOp2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_examples = 5\n",
        "\n",
        "for ax, ele in zip(axs, test_dataset.take(n_examples)):\n",
        "\n",
        "    image, y_true = ele\n",
        "    prediction = model.predict(image)[0]\n",
        "    prediction = OtsuFilter(prediction)\n",
        "\n",
        "    imshow(image[0])\n",
        "    plt.title('Original image')\n",
        "    plt.show()\n",
        "    imshow(np.squeeze(mask[0]))\n",
        "    plt.title('Original mask')\n",
        "    plt.show()\n",
        "    imshow(np.squeeze(prediction))\n",
        "    plt.title('Predicted area')\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "j8zVepLkZWdU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "meanIoU = MeanIoU(num_classes=2)\n",
        "for (image, y_true) in tqdm(test_dataset.take(test_set_size)):\n",
        "    prediction = model.predict(image)[0]\n",
        "    prediction = OtsuFilter(prediction)\n",
        "    meanIoU.update_state(y_true[0], prediction)\n",
        "print(meanIoU.result().numpy())"
      ],
      "metadata": {
        "id": "b4JSihbvaMFL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}